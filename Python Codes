Data Science Python Codes Cheatsheet

#-------------------------------------------------------------------------------------------------------------------------------------------------
#Normalize or scale data
1) Log normalization for huge variation in data
df['col'] = np.log(df['col'])

2) Standard Normalize Scaling
from sklearn.preprocessing import StandardScale
scaler = StandardScalar
df_scaler = pd.DataFrame(Scaler.fit_transform(df), columns = df.columns)

3)Scale
from sklearn.preprocessing import Scale
X_scale = scale(x)
np.mean(x)
np.std(x)
np.max(x)
np.std(x_scaled)


#-------------------------------------------------------------------------------------------------------------------------------------------------
#Imput missing data
from sklearn.preprocessing import Imputer
imp=Imputer(missing_value = 'NAN', strfy = 'mean', axis=0)

imp.fit(X)
X = imp.fit_transform(X)

#-------------------------------------------------------------------------------------------------------------------------------------------------
#Encoding the columns

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['columns'] = le.fit_transform(df['columns'])

from sklearn.preprocessing import OneHotEncoder
pd.get_dummies(df['columns'])

#-------------------------------------------------------------------------------------------------------------------------------------------------
#Split the data to train and test

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 222, stratify=23)

#-------------------------------------------------------------------------------------------------------------------------------------------------
#SMOTE for imbalance data - synthetic memory overfitting technic on the training data only

X = data_final.loc[:, data_final.columns != 'y']
y = data_final.loc[:, data_final.columns == 'y']

from imblearn.over_sampling import SMOTE
os = SMOTE(random_state=0)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
columns = X_train.columns
os_data_X,os_data_y=os.fit_sample(X_train, y_train)
os_data_X = pd.DataFrame(data=os_data_X,columns=columns )
os_data_y= pd.DataFrame(data=os_data_y,columns=['y'])

# we can Check the numbers of our data
print("length of oversampled data is ",len(os_data_X))
print("Number of no subscription in oversampled data",len(os_data_y[os_data_y['y']==0]))
print("Number of subscription",len(os_data_y[os_data_y['y']==1]))
print("Proportion of no subscription data in oversampled data is ",len(os_data_y[os_data_y['y']==0])/len(os_data_X))
print("Proportion of subscription data in oversampled data is ",len(os_data_y[os_data_y['y']==1])/len(os_data_X))

#-------------------------------------------------------------------------------------------------------------------------------------------------
#Hyperparameter model using GridsearchCV

from sklearn.model_selection import GridSearchCV
param = {'n_neighbour': np.arange(1,50)}

knn = kneighborClassifier()

knn_cv = GridSearch(knn, param_grid, CV=5)

knn_cv.fit(x, y)

knn_cv.best_params_
knn_cv.best_score_
knn_cv.score(x_test, y_test)

#-------------------------------------------------------------------------------------------------------------------------------------------------
#Confusion Matrix
from sklearn.metric import confusion_matrix
confusion_matrix(y_test, y_pred)

#ROC Curve
from sklearn.metrics import roc_curve

##after fitting
y_pred_prob = logreg.predict_prob(x_test)[,:1]

fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)

#Plotting
plt.plot([0,1], [0,1], 'k--')
plt.plot(fpr, tpr, label='Logistic Regression')

#AUC ROC
from sklearn.metric import roc-auc_score

##after fitting
y_pred_prob = logreg.predictprob(x_test)[:,1]
roc_auc_score(y_test, y_pred_prob)


#-------------------------------------------------------------------------------------------------------------------------------------------------
#Accuracy Score
from sklearn.metrics import accuracy_score


#-------------------------------------------------------------------------------------------------------------------------------------------------

#Models
#1) Linear Regression

#Package
from sklearn.linear_model import LinearRegression
lm = LinearRegression(fit_intercept = True)

#Model Fit
model = lm.fit(x_train, y_train)

#Intercept and slope value
intercept = model.intercept
slope = model.coef_[0]

#Predicting the model
y_pred = model.predict(x_test)

#Evaluating the model
#RMSE
from sklearn import metrics
print('RMSE: ', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

#R2
from sklearn.metrics import r2_score as r2
r2(y_test, y_pred)

#-------------------------------------------------------------------------------------------------------------------------------------------------

2) Logistic Regression

#Package
from sklearn.linear_model import LogisticRegression
from sklearn import metrics

#model
logreg = LogisticRegression(random_state=1)
logreg.fit(X_train, y_train)

#Predicting
y_pred = logreg.predict(X_test)
print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))

#To plot the decision boundary
plot_labeled_decision_regions(X_test, y_test, logreg)

#-------------------------------------------------------------------------------------------------------------------------------------------------

3) Support Vector Machine

#Linear SVC
#Package
from sklearn.svm import LinearSVC
svm = LinearSVC()

#Model 
model = svm.fit(x_train, y_train)
model.score(x_test, y_test)




#SVC

#Package
from sklearn.svm import SVC
svm = SVC()

#Hyperparameter tuning
from sklearn.model_selection import GridSearchCV
parameter = {'gamma': [0.00001, 0.0001, 0.001, 0.01, 0.1], 'c': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}
svm_searcher = gridsearchCV(svm, parameters)
svm_searcher.fit(x, y)
svm_searcher.best_params_
svm_searcher.best_score_

svm_searcher.score(x_test, y_test)


#Model 
model = svm.fit(x_train, y_train)
model.score(x_test, y_test)

#Plot the model
plot_classifier(x, y, svm, linear=(11,15,0,6))

#-------------------------------------------------------------------------------------------------------------------------------------------------

#4)Schostic Gradient Decent Model for Logistic and SVM for larger dataset

#model
from sklearn.linear_model import SGDClassifier
logreg = SGDClassifier(loss='log')
linsvm = SGDClassifier(loss='hinge')

#Hyperparameter
model = SGDClassifier(random_state = 0)
parameter = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], 'loss': ['log', 'hinge'], 'penalty': ['l1', 'l2']}

search = GridSearchCV(model, parameter, cv=10)
search.fit(x_train, y_train)
search.best_params_
search.best_score_
search.score(x_test, y_test)

#-------------------------------------------------------------------------------------------------------------------------------------------------
#5) Decision Tree Classifier
#import the model
from sklearn.tree import DecisionTreeClassifier
DT = DecisionTreeClassifier(criterion='gini', max_depth=2, random_state=1) ## criterion='entropy'

# Define params_dt
params_dt = {
             'max_depth': [2, 3, 4],
             'min_samples_leaf': [0.12, 0.14, 0.16, 0.18]
            }
# Import GridSearchCV
from sklearn.model_selection import GridSearchCV

# Instantiate grid_dt
grid_dt = GridSearchCV(estimator=dt,
                       param_grid=params_dt,
                       scoring='roc_auc',
                       cv=5,
                       n_jobs=-1)
					   
 # Import roc_auc_score from sklearn.metrics 
from sklearn.metrics import roc_auc_score

# Extract the best estimator
best_model = grid_dt.best_estimator_

# Predict the test set probabilities of the positive class
y_pred_proba = best_model.predict_proba(X_test)[:,1]

# Compute test_roc_auc
test_roc_auc = roc_auc_score(y_test, y_pred_proba)

# Print test_roc_auc
print('Test set ROC AUC score: {:.3f}'.format(test_roc_auc))

#Fitting the model
dt.fit(x_test, y_test)

#Predicting the model
y_pred = dt.predict(x_test)	

#Evaluate the test set accuracy
accuracy_score(y_test, y_pred)
print("Test set accuracy: {:.2f}".format(acc))

#To plot the decision boundary
plot_labeled_decision_regions(X_test, y_test, dt)

#Decision Tree Regression
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error as MSE

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 222, stratify=23)

from sklearn.model_selection import cross_val_score
SEED = 123

dt = DecisionTreeRegressor(max_depth=4, min_samples_leaf=0.1, random_state=SEED)
#evaluate the list of MSE obtained by 10-fold CV
#set the n_jobs =-1 to exploit all the CPU cores in computation
MSE_CV = - cross_val_score(dt, x_train, y_train, cv=10, scoring='neg_mean_squared_error', n_jobs=-1)

#Fitting the model
dt.fit(x_test, y_test)

#Predicting the model
y_pred = dt.predict(x_test)	

#CV MSE
print('CV MSE: ' MSE_CV.mean())

#Training set MSE
print('Train MSE', MSE(y_train, y_pred))

#Tesst set MSE
print('Test MSE', MSE(y_test, y_pred))

#Evaluating the model
#RMSE
from sklearn import metrics
print('RMSE: ', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

#-------------------------------------------------------------------------------------------------------------------------------------------------
6) Baggin Classifier

# Import DecisionTreeClassifier
from sklearn.tree import DecisionTreeClassifier

# Import BaggingClassifier
from sklearn.ensemble import BaggingClassifier

# Instantiate dt
dt = DecisionTreeClassifier(random_state=1)

# Instantiate bc
bc = BaggingClassifier(base_estimator=dt, n_estimators=50, oob_score = True, random_state=1)

# Fit bc to the training set
bc.fit(X_train, y_train)

# Predict test set labels
y_pred = bc.predict(X_test)

# Evaluate test set accuracy
acc_test = accuracy_score(y_test, y_pred)

# Evaluate OOB accuracy
acc_oob = bc.oob_score_

# Print acc_test and acc_oob
print('Test set accuracy: {:.3f}, OOB accuracy: {:.3f}'.format(acc_test, acc_oob))

#-------------------------------------------------------------------------------------------------------------------------------------------------
7) Random Forest

#classifire
# Import RandomForestRegressor
from sklearn.ensemble import RandomForestClassifier

#Regression
# Import RandomForestRegressor
from sklearn.ensemble import RandomForestRegressor

# Instantiate rf
rf = RandomForestRegressor(n_estimators=25, random_state=2)

# Define the dictionary 'params_rf'
params_rf = {
             'n_estimators': [100, 350, 500],
             'max_features': ['log2', 'auto', 'sqrt'],
             'min_samples_leaf': [2, 10, 30], 
             }

# Import GridSearchCV
from sklearn.model_selection import  GridSearchCV

# Instantiate grid_rf
grid_rf = GridSearchCV(estimator=rf,
                       param_grid=params_rf,
                       scoring='neg_mean_squared_error',
                       cv=3,
                       verbose=1,
                       n_jobs=-1)

# Extract the best estimator
best_model = grid_rf.best_estimator_

# Predict test set labels
y_pred = best_model.predict(X_test)
					   
# Fit rf to the training set            
rf.fit(X_train, y_train)   

# Import mean_squared_error as MSE
from sklearn.metrics import mean_squared_error as MSE

# Predict the test set labels
y_pred = rf.predict(X_test)

# Evaluate the test set RMSE
rmse_test = MSE(y_test, y_pred)**(1/2)

# Print rmse_test
print('Test set RMSE of rf: {:.2f}'.format(rmse_test))

# Create a pd.Series of features importances
importances = pd.Series(data=rf.feature_importances_, index= X_train.columns)

# Sort importances
importances_sorted = importances.sort_values()

# Draw a horizontal barplot of importances_sorted
importances_sorted.plot(kind='barh', color='lightgreen')
plt.title('Features Importances')
plt.show()

#-------------------------------------------------------------------------------------------------------------------------------------------------
8) AdaBoost

#Classifier
# Import DecisionTreeClassifier
from sklearn.tree import DecisionTreeClassifier

# Import AdaBoostClassifier
from sklearn.ensemble import AdaBoostClassifier

# Instantiate dt
dt = DecisionTreeClassifier(max_depth=2, random_state=1)

# Instantiate ada
ada = AdaBoostClassifier(base_estimator=dt, n_estimators=180, random_state=1)

# Fit ada to the training set
ada.fit(X_train, y_train)

# Compute the probabilities of obtaining the positive class
y_pred_proba = ada.predict_proba(X_test)[:,1]

# Import roc_auc_score
from sklearn.metrics import roc_auc_score

# Evaluate test-set roc_auc_score
ada_roc_auc = roc_auc_score(y_test, y_pred_proba)

# Print roc_auc_score
print('ROC AUC score: {:.2f}'.format(ada_roc_auc))

#Regression
# Import AdaBoostRegressor
from sklearn.ensemble import AdaBoostRegressor

#-------------------------------------------------------------------------------------------------------------------------------------------------
9) Gradient Boosting

# Import GradientBoostingRegressor
from sklearn.ensemble import GradientBoostingRegressor

# Instantiate gb
gb = GradientBoostingRegressor(max_depth=4, n_estimators=200, random_state=2)

# Fit gb to the training set
gb.fit(X_train, y_train)

# Predict test set labels
y_pred = gb.predict(X_test)

# Import mean_squared_error as MSE
from sklearn.metrics import mean_squared_error as MSE

# Compute MSE
mse_test = MSE(y_test, y_pred)

# Compute RMSE
rmse_test = mse_test**(1/2)

# Print RMSE
print('Test set RMSE of gb: {:.3f}'.format(rmse_test))

#-------------------------------------------------------------------------------------------------------------------------------------------------
10) Stochastic Gradient Boosting

# Import GradientBoostingRegressor
from sklearn.ensemble import GradientBoostingRegressor

# Instantiate sgbr
sgbr = GradientBoostingRegressor(max_depth=4, subsample=0.9, max_features=0.75, n_estimators=200, random_state=2)

# Fit sgbr to the training set
sgbr.fit(X_train, y_train)

# Predict test set labels
y_pred = sgbr.predict(X_test)

# Import mean_squared_error as MSE
from sklearn.metrics import mean_squared_error as MSE

# Compute test set MSE
mse_test = MSE(y_test, y_pred)

# Compute test set RMSE
rmse_test = mse_test**(1/2)

# Print rmse_test
print('Test set RMSE of sgbr: {:.3f}'.format(rmse_test))
